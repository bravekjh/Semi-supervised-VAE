{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import logpdf\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(19538)\n",
    "random.seed(19538)\n",
    "tf.set_random_seed(19538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tcl\n",
    "class VAE:\n",
    "    def __init__(self):\n",
    "        self.name = 'VAE'\n",
    "        \n",
    "        # classifier params\n",
    "        self.hidden_size = 600\n",
    "        \n",
    "        # encode & decoder params\n",
    "        self.z_dim = 50\n",
    "        self.x_dim = 28*28\n",
    "        \n",
    "        # training\n",
    "        # learning rate\n",
    "        self.lr_decay_factor = 0.95\n",
    "        self.learning_rate = 1e-3\n",
    "        # first moment decay\n",
    "        self.beta1 = 1-1e-1\n",
    "        # second moment decay\n",
    "        self.beta2 = 1-1e-3\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._build_train_op()\n",
    "        self.check_parameters()\n",
    "    \n",
    "    def check_parameters(self):\n",
    "        for var in tf.trainable_variables():\n",
    "            print('%s: %s' % (var.name, var.get_shape()))\n",
    "        print()\n",
    "    \n",
    "    def get_collection(self, collections):\n",
    "        return [var for var in tf.get_collection(collections)]\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        batch_size, eps_dim = tf.shape(mu)[0], tf.shape(mu)[1]\n",
    "        std = tf.exp(logvar * 0.5)\n",
    "        eps = tf.random_normal([batch_size, eps_dim])\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def encode(self, x, reuse = False):\n",
    "        with tf.variable_scope(self.name+'/encoder', reuse = reuse):\n",
    "            h1 = tcl.fully_connected(x,  self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob = self.encode_keep_prob)\n",
    "            h2 = tcl.fully_connected(h1, self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            h2 = tf.nn.dropout(h2, keep_prob = self.encode_keep_prob)\n",
    "            mu     = tcl.fully_connected(h2, self.z_dim, activation_fn = None)\n",
    "            logvar = tcl.fully_connected(h2, self.z_dim, activation_fn = None)\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            return z, mu, logvar\n",
    "    \n",
    "    def decode(self, z, reuse = False):\n",
    "        with tf.variable_scope(self.name+'/decoder', reuse = reuse):\n",
    "            h1 = tcl.fully_connected(z,  self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob = self.decode_keep_prob)\n",
    "            h2 = tcl.fully_connected(h1, self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            h2 = tf.nn.dropout(h2, keep_prob = self.decode_keep_prob)\n",
    "            x  = tcl.fully_connected(h2, self.x_dim, activation_fn = tf.nn.softmax) \n",
    "            return x\n",
    "        \n",
    "    def L(self, x, recon_x, z, mu_z, logvar_z):\n",
    "        # (batch_size, z_dim) -> batch_size,\n",
    "        kld = tf.reduce_sum(logpdf.KLD(mu_z, logvar_z), 1)\n",
    "        # (batch_size, 784)   -> batch_size,\n",
    "        logpx = tf.reduce_sum(logpdf.bernoulli(recon_x, x), 1)\n",
    "        loss = kld - logpx\n",
    "        return loss\n",
    "    \n",
    "    def _build_graph(self, reuse = False):\n",
    "        self.x = tf.placeholder(tf.float32, shape = (None, self.x_dim))\n",
    "        self.encode_keep_prob = tf.placeholder(tf.float32)\n",
    "        self.decode_keep_prob = tf.placeholder(tf.float32)\n",
    "        '''\n",
    "            labelled data, encoder & decoder\n",
    "        '''\n",
    "        # encoder, labelled data\n",
    "        self.z, self.mu_z, self.logvar_z = self.encode(self.x, reuse = reuse)\n",
    "\n",
    "        # decoder, labelled data\n",
    "        self.x_recon = self.decode(self.z, reuse = reuse)\n",
    "        \n",
    "        # loss of labelled data, refered as L(x, y)\n",
    "        self.loss = self.L(self.x, self.x_recon, self.z, self.mu_z, self.logvar_z)\n",
    "        self.loss = tf.reduce_mean(self.loss, 0)\n",
    "\n",
    "        trainable_vars_key = tf.GraphKeys.TRAINABLE_VARIABLES\n",
    "        encoder_vars = tf.get_collection(key=trainable_vars_key, scope=self.name+\"/encoder\")\n",
    "        decoder_vars = tf.get_collection(key=trainable_vars_key, scope=self.name+\"/decoder\")\n",
    "        tcl.apply_regularization(tcl.l2_regularizer(1.0), encoder_vars)\n",
    "        tcl.apply_regularization(tcl.l2_regularizer(1.0), decoder_vars)\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_constant = 1e-6\n",
    "        self.loss += reg_constant * tf.reduce_sum(reg_losses)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=3,\\\n",
    "            pad_step_number=True, keep_checkpoint_every_n_hours=5.0)\n",
    "            \n",
    "    def _build_train_op(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable = False)\n",
    "        self.lr = tf.Variable(self.learning_rate, trainable=False, \n",
    "                    dtype=tf.float32)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        def ClipIfNotNone(grad):\n",
    "            if grad is None:\n",
    "                return grad\n",
    "            return tf.clip_by_value(grad, -1, 1)\n",
    "        capped_gvs = [(ClipIfNotNone(grad), var) for grad, var in grads_and_vars]\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs, self.global_step)\n",
    "        self.lr_decay_op = self.lr.assign(\n",
    "                self.lr * self.lr_decay_factor)\n",
    "        \n",
    "    def lr_decay(self, sess):\n",
    "        _ = sess.run([self.lr_decay_op])\n",
    "    \n",
    "    def _build_generator_op(self):\n",
    "        self._z   = tf.placeholder(tf.float32, shape = (None, self.z_dim))\n",
    "        self._gen = self.decode(self._z, reuse = True)\n",
    "    \n",
    "    def optimize(self, sess, x):\n",
    "        feed_dict = {\n",
    "            self.x: x,\n",
    "            self.encode_keep_prob: 1,\n",
    "            self.decode_keep_prob: 1,\n",
    "        }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict = feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def recon_x(self, sess, x):\n",
    "        feed_dict = {\n",
    "            self.x: x,\n",
    "            self.encode_keep_prob: 1,\n",
    "            self.decode_keep_prob: 1,\n",
    "        }\n",
    "        x_recon = sess.run([self.x_recon], feed_dict = feed_dict)[0]\n",
    "        return x_recon\n",
    "    \n",
    "    def generate(self, sess, z):\n",
    "        feed_dict = {\n",
    "            self._z : z,\n",
    "            self.decode_keep_prob: 1,\n",
    "        }\n",
    "        _gen = sess.run([self._gen], feed_dict = feed_dict)[0]\n",
    "        return _gen\n",
    "    \n",
    "    def save(self, sess, path = 'models/vae/ckpt'):\n",
    "        self.saver.save(sess, path, global_step = self.global_step)\n",
    "    \n",
    "    def get_repr(self, sess, x, stats_only = False):\n",
    "        feed_dict = {\n",
    "            self.x: x,\n",
    "            self.encode_keep_prob: 1,\n",
    "            self.decode_keep_prob: 1,\n",
    "        }\n",
    "        if stats_only:\n",
    "            mu, logvar = sess.run([self.mu_z, self.logvar_z], feed_dict = feed_dict)\n",
    "            return mu, logvar\n",
    "        else:\n",
    "            z = sess.run([self.z], feed_dict = feed_dict)[0]\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE/encoder/fully_connected/weights:0: (784, 600)\n",
      "VAE/encoder/fully_connected/biases:0: (600,)\n",
      "VAE/encoder/fully_connected_1/weights:0: (600, 600)\n",
      "VAE/encoder/fully_connected_1/biases:0: (600,)\n",
      "VAE/encoder/fully_connected_2/weights:0: (600, 50)\n",
      "VAE/encoder/fully_connected_2/biases:0: (50,)\n",
      "VAE/encoder/fully_connected_3/weights:0: (600, 50)\n",
      "VAE/encoder/fully_connected_3/biases:0: (50,)\n",
      "VAE/decoder/fully_connected/weights:0: (50, 600)\n",
      "VAE/decoder/fully_connected/biases:0: (600,)\n",
      "VAE/decoder/fully_connected_1/weights:0: (600, 600)\n",
      "VAE/decoder/fully_connected_1/biases:0: (600,)\n",
      "VAE/decoder/fully_connected_2/weights:0: (600, 784)\n",
      "VAE/decoder/fully_connected_2/biases:0: (784,)\n",
      "\n",
      "Time: 0 m 4 s\n",
      "Iteration 500/50000\n",
      "loss: 573.53\n",
      "\n",
      "Time: 0 m 8 s\n",
      "Iteration 1000/50000\n",
      "loss: 553.91\n",
      "\n",
      "Time: 0 m 11 s\n",
      "Iteration 1500/50000\n",
      "loss: 554.72\n",
      "\n",
      "Time: 0 m 15 s\n",
      "Iteration 2000/50000\n",
      "loss: 558.06\n",
      "\n",
      "Time: 0 m 19 s\n",
      "Iteration 2500/50000\n",
      "loss: 547.06\n",
      "\n",
      "Time: 0 m 22 s\n",
      "Iteration 3000/50000\n",
      "loss: 584.04\n",
      "\n",
      "Time: 0 m 26 s\n",
      "Iteration 3500/50000\n",
      "loss: 529.55\n",
      "\n",
      "Time: 0 m 29 s\n",
      "Iteration 4000/50000\n",
      "loss: 547.92\n",
      "\n",
      "Time: 0 m 33 s\n",
      "Iteration 4500/50000\n",
      "loss: 552.16\n",
      "\n",
      "Time: 0 m 37 s\n",
      "Iteration 5000/50000\n",
      "loss: 543.47\n",
      "\n",
      "Time: 0 m 40 s\n",
      "Iteration 5500/50000\n",
      "loss: 561.60\n",
      "\n",
      "Time: 0 m 44 s\n",
      "Iteration 6000/50000\n",
      "loss: 548.55\n",
      "\n",
      "Time: 0 m 47 s\n",
      "Iteration 6500/50000\n",
      "loss: 533.64\n",
      "\n",
      "Time: 0 m 51 s\n",
      "Iteration 7000/50000\n",
      "loss: 545.54\n",
      "\n",
      "Time: 0 m 54 s\n",
      "Iteration 7500/50000\n",
      "loss: 545.12\n",
      "\n",
      "Time: 0 m 58 s\n",
      "Iteration 8000/50000\n",
      "loss: 551.93\n",
      "\n",
      "Time: 1 m 2 s\n",
      "Iteration 8500/50000\n",
      "loss: 498.13\n",
      "\n",
      "Time: 1 m 5 s\n",
      "Iteration 9000/50000\n",
      "loss: 560.90\n",
      "\n",
      "Time: 1 m 9 s\n",
      "Iteration 9500/50000\n",
      "loss: 564.57\n",
      "\n",
      "Time: 1 m 12 s\n",
      "Iteration 10000/50000\n",
      "loss: 526.61\n",
      "\n",
      "Time: 1 m 16 s\n",
      "Iteration 10500/50000\n",
      "loss: 499.46\n",
      "\n",
      "Time: 1 m 20 s\n",
      "Iteration 11000/50000\n",
      "loss: 540.08\n",
      "\n",
      "Time: 1 m 23 s\n",
      "Iteration 11500/50000\n",
      "loss: 553.12\n",
      "\n",
      "Time: 1 m 27 s\n",
      "Iteration 12000/50000\n",
      "loss: 530.89\n",
      "\n",
      "Time: 1 m 30 s\n",
      "Iteration 12500/50000\n",
      "loss: 547.61\n",
      "\n",
      "Time: 1 m 34 s\n",
      "Iteration 13000/50000\n",
      "loss: 542.86\n",
      "\n",
      "Time: 1 m 37 s\n",
      "Iteration 13500/50000\n",
      "loss: 534.58\n",
      "\n",
      "Time: 1 m 41 s\n",
      "Iteration 14000/50000\n",
      "loss: 554.87\n",
      "\n",
      "Time: 1 m 45 s\n",
      "Iteration 14500/50000\n",
      "loss: 554.92\n",
      "\n",
      "Time: 1 m 48 s\n",
      "Iteration 15000/50000\n",
      "loss: 547.54\n",
      "\n",
      "Time: 1 m 52 s\n",
      "Iteration 15500/50000\n",
      "loss: 543.12\n",
      "\n",
      "Time: 1 m 55 s\n",
      "Iteration 16000/50000\n",
      "loss: 535.54\n",
      "\n",
      "Time: 1 m 59 s\n",
      "Iteration 16500/50000\n",
      "loss: 546.64\n",
      "\n",
      "Time: 2 m 2 s\n",
      "Iteration 17000/50000\n",
      "loss: 590.43\n",
      "\n",
      "Time: 2 m 6 s\n",
      "Iteration 17500/50000\n",
      "loss: 503.61\n",
      "\n",
      "Time: 2 m 10 s\n",
      "Iteration 18000/50000\n",
      "loss: 568.37\n",
      "\n",
      "Time: 2 m 13 s\n",
      "Iteration 18500/50000\n",
      "loss: 523.97\n",
      "\n",
      "Time: 2 m 17 s\n",
      "Iteration 19000/50000\n",
      "loss: 565.50\n",
      "\n",
      "Time: 2 m 20 s\n",
      "Iteration 19500/50000\n",
      "loss: 520.21\n",
      "\n",
      "Time: 2 m 24 s\n",
      "Iteration 20000/50000\n",
      "loss: 513.63\n",
      "\n",
      "Time: 2 m 28 s\n",
      "Iteration 20500/50000\n",
      "loss: 554.77\n",
      "\n",
      "Time: 2 m 31 s\n",
      "Iteration 21000/50000\n",
      "loss: 521.86\n",
      "\n",
      "Time: 2 m 35 s\n",
      "Iteration 21500/50000\n",
      "loss: 517.36\n",
      "\n",
      "Time: 2 m 38 s\n",
      "Iteration 22000/50000\n",
      "loss: 548.15\n",
      "\n",
      "Time: 2 m 42 s\n",
      "Iteration 22500/50000\n",
      "loss: 542.33\n",
      "\n",
      "Time: 2 m 45 s\n",
      "Iteration 23000/50000\n",
      "loss: 517.84\n",
      "\n",
      "Time: 2 m 49 s\n",
      "Iteration 23500/50000\n",
      "loss: 490.80\n",
      "\n",
      "Time: 2 m 53 s\n",
      "Iteration 24000/50000\n",
      "loss: 492.08\n",
      "\n",
      "Time: 2 m 56 s\n",
      "Iteration 24500/50000\n",
      "loss: 544.01\n",
      "\n",
      "Time: 3 m 0 s\n",
      "Iteration 25000/50000\n",
      "loss: 574.87\n",
      "\n",
      "Time: 3 m 3 s\n",
      "Iteration 25500/50000\n",
      "loss: 555.62\n",
      "\n",
      "Time: 3 m 7 s\n",
      "Iteration 26000/50000\n",
      "loss: 545.45\n",
      "\n",
      "Time: 3 m 10 s\n",
      "Iteration 26500/50000\n",
      "loss: 554.99\n",
      "\n",
      "Time: 3 m 14 s\n",
      "Iteration 27000/50000\n",
      "loss: 539.20\n",
      "\n",
      "Time: 3 m 18 s\n",
      "Iteration 27500/50000\n",
      "loss: 559.10\n",
      "\n",
      "Time: 3 m 21 s\n",
      "Iteration 28000/50000\n",
      "loss: 571.53\n",
      "\n",
      "Time: 3 m 25 s\n",
      "Iteration 28500/50000\n",
      "loss: 548.60\n",
      "\n",
      "Time: 3 m 28 s\n",
      "Iteration 29000/50000\n",
      "loss: 561.82\n",
      "\n",
      "Time: 3 m 32 s\n",
      "Iteration 29500/50000\n",
      "loss: 521.38\n",
      "\n",
      "Time: 3 m 35 s\n",
      "Iteration 30000/50000\n",
      "loss: 534.40\n",
      "\n",
      "Time: 3 m 39 s\n",
      "Iteration 30500/50000\n",
      "loss: 535.37\n",
      "\n",
      "Time: 3 m 43 s\n",
      "Iteration 31000/50000\n",
      "loss: 563.27\n",
      "\n",
      "Time: 3 m 46 s\n",
      "Iteration 31500/50000\n",
      "loss: 524.17\n",
      "\n",
      "Time: 3 m 50 s\n",
      "Iteration 32000/50000\n",
      "loss: 507.75\n",
      "\n",
      "Time: 3 m 53 s\n",
      "Iteration 32500/50000\n",
      "loss: 566.34\n",
      "\n",
      "Time: 3 m 57 s\n",
      "Iteration 33000/50000\n",
      "loss: 523.77\n",
      "\n",
      "Time: 4 m 0 s\n",
      "Iteration 33500/50000\n",
      "loss: 530.11\n",
      "\n",
      "Time: 4 m 4 s\n",
      "Iteration 34000/50000\n",
      "loss: 552.92\n",
      "\n",
      "Time: 4 m 8 s\n",
      "Iteration 34500/50000\n",
      "loss: 543.41\n",
      "\n",
      "Time: 4 m 11 s\n",
      "Iteration 35000/50000\n",
      "loss: 522.20\n",
      "\n",
      "Time: 4 m 15 s\n",
      "Iteration 35500/50000\n",
      "loss: 537.37\n",
      "\n",
      "Time: 4 m 18 s\n",
      "Iteration 36000/50000\n",
      "loss: 535.04\n",
      "\n",
      "Time: 4 m 22 s\n",
      "Iteration 36500/50000\n",
      "loss: 520.69\n",
      "\n",
      "Time: 4 m 26 s\n",
      "Iteration 37000/50000\n",
      "loss: 542.20\n",
      "\n",
      "Time: 4 m 29 s\n",
      "Iteration 37500/50000\n",
      "loss: 549.87\n",
      "\n",
      "Time: 4 m 33 s\n",
      "Iteration 38000/50000\n",
      "loss: 534.24\n",
      "\n",
      "Time: 4 m 36 s\n",
      "Iteration 38500/50000\n",
      "loss: 529.06\n",
      "\n",
      "Time: 4 m 40 s\n",
      "Iteration 39000/50000\n",
      "loss: 553.39\n",
      "\n",
      "Time: 4 m 43 s\n",
      "Iteration 39500/50000\n",
      "loss: 569.84\n",
      "\n",
      "Time: 4 m 47 s\n",
      "Iteration 40000/50000\n",
      "loss: 511.82\n",
      "\n",
      "Time: 4 m 51 s\n",
      "Iteration 40500/50000\n",
      "loss: 511.64\n",
      "\n",
      "Time: 4 m 54 s\n",
      "Iteration 41000/50000\n",
      "loss: 552.65\n",
      "\n",
      "Time: 4 m 58 s\n",
      "Iteration 41500/50000\n",
      "loss: 543.02\n",
      "\n",
      "Time: 5 m 1 s\n",
      "Iteration 42000/50000\n",
      "loss: 533.17\n",
      "\n",
      "Time: 5 m 5 s\n",
      "Iteration 42500/50000\n",
      "loss: 524.77\n",
      "\n",
      "Time: 5 m 9 s\n",
      "Iteration 43000/50000\n",
      "loss: 515.52\n",
      "\n",
      "Time: 5 m 12 s\n",
      "Iteration 43500/50000\n",
      "loss: 481.50\n",
      "\n",
      "Time: 5 m 16 s\n",
      "Iteration 44000/50000\n",
      "loss: 488.30\n",
      "\n",
      "Time: 5 m 19 s\n",
      "Iteration 44500/50000\n",
      "loss: 554.49\n",
      "\n",
      "Time: 5 m 23 s\n",
      "Iteration 45000/50000\n",
      "loss: 548.03\n",
      "\n",
      "Time: 5 m 27 s\n",
      "Iteration 45500/50000\n",
      "loss: 550.01\n",
      "\n",
      "Time: 5 m 30 s\n",
      "Iteration 46000/50000\n",
      "loss: 543.01\n",
      "\n",
      "Time: 5 m 34 s\n",
      "Iteration 46500/50000\n",
      "loss: 546.43\n",
      "\n",
      "Time: 5 m 37 s\n",
      "Iteration 47000/50000\n",
      "loss: 527.83\n",
      "\n",
      "Time: 5 m 41 s\n",
      "Iteration 47500/50000\n",
      "loss: 507.17\n",
      "\n",
      "Time: 5 m 44 s\n",
      "Iteration 48000/50000\n",
      "loss: 512.34\n",
      "\n",
      "Time: 5 m 48 s\n",
      "Iteration 48500/50000\n",
      "loss: 567.99\n",
      "\n",
      "Time: 5 m 52 s\n",
      "Iteration 49000/50000\n",
      "loss: 522.86\n",
      "\n",
      "Time: 5 m 55 s\n",
      "Iteration 49500/50000\n",
      "loss: 500.16\n",
      "\n",
      "Time: 5 m 59 s\n",
      "Iteration 50000/50000\n",
      "loss: 537.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 50000\n",
    "n_batch_size = 100\n",
    "n_epoch      = 100\n",
    "max_iter = dataset_size/n_batch_size*n_epoch\n",
    "from utils import (load_mnist, batch_generator, time_since)\n",
    "MNIST_PATH = './data/mnist_28.pkl.gz'\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = load_mnist(MNIST_PATH)\n",
    "batch_gen = batch_generator(zip(train_x), n_batch_size, n_epoch)\n",
    "\n",
    "gen_x = []\n",
    "\n",
    "# Set config for tensorflow session.\n",
    "tf_config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 1}, # single gpu\n",
    ")\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "with tf.Session() as sess:\n",
    "    model = VAE()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start = time.time()\n",
    "    for x_batch in batch_gen:\n",
    "        x_batch = zip(*x_batch)[0]\n",
    "        loss = model.optimize(sess, x_batch)\n",
    "        \n",
    "        if(model.global_step.eval() % 500 == 0):\n",
    "            print('Time: %s'        % time_since(start))\n",
    "            print('Iteration %d/%d' % (model.global_step.eval(), max_iter))\n",
    "            print('loss: %.2f'      % loss)\n",
    "            print()\n",
    "            \n",
    "            # save cpkt\n",
    "            model.save(sess)\n",
    "\n",
    "        if model.global_step.eval() % 1000 == 0:\n",
    "            model.lr_decay(sess)\n",
    "            \n",
    "            # save imgs\n",
    "            # x_recon = model.recon_x(sess, x_batch)\n",
    "            # gen_x.append(x_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gen_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e41a75895e4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gen_x[25][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "def display(x):\n",
    "    x = np.clip(x, 0, 1)\n",
    "    nrows, ncols = 10, 10\n",
    "    n_count  = 0\n",
    "    fig, axarr = plt.subplots(nrows = nrows, ncols = ncols)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            axarr[i, j].axis('off')\n",
    "            axarr[i, j].imshow(x[n_count], cmap='gray')\n",
    "            n_count += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(np.reshape(gen_x[80], (100,28,28)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
