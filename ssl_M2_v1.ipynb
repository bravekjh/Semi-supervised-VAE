{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import notebook_util\n",
    "notebook_util.setup_one_gpu()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import logpdf\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(31415)\n",
    "tf.set_random_seed(31415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as tcl\n",
    "class SSL:\n",
    "    def __init__(self, x_dim = 50):\n",
    "        self.name = 'SSL'\n",
    "        \n",
    "        # classifier params\n",
    "        self.hidden_size = 500\n",
    "        self.num_labels = 10\n",
    "        \n",
    "        # encode & decoder params\n",
    "        self.z_dim = 50\n",
    "        self.x_dim = x_dim\n",
    "        self.batch_size   = 500\n",
    "        self.dataset_size = 50000\n",
    "        \n",
    "        # training\n",
    "        self.lr_decay_factor = 0.95\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build_annealing()\n",
    "            self._build_graph()\n",
    "            self._build_train_op()\n",
    "        self.check_parameters()\n",
    "    \n",
    "    def global_vars(self):\n",
    "        var_list = [var for var in tf.global_variables() if self.name in var.name]\n",
    "        return var_list\n",
    "    \n",
    "    def trainable_vars(self):\n",
    "        var_list = [var for var in tf.trainable_variables() if self.name in var.name]\n",
    "        return var_list\n",
    "    \n",
    "    def check_parameters(self):\n",
    "        for var in tf.trainable_variables():\n",
    "            print('%s: %s' % (var.name, var.get_shape()))\n",
    "        print()\n",
    "    \n",
    "    def get_collection(self, collections):\n",
    "        return [var for var in tf.get_collection(collections)]\n",
    "    \n",
    "    def classify(self, x, reuse = False):\n",
    "        with tf.variable_scope('classifier', reuse = reuse):\n",
    "            h = tcl.fully_connected(x, self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            y = tcl.fully_connected(h, self.num_labels,  activation_fn = tf.nn.softplus)\n",
    "            return y\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = tf.exp(logvar * 0.5)\n",
    "        eps = tf.random_normal(tf.shape(mu))\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def encode(self, x, y, reuse = False):\n",
    "        with tf.variable_scope('encoder', reuse = reuse):\n",
    "            concat = tf.concat([x, y], 1)\n",
    "            h = tcl.fully_connected(concat, self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            mu     = tcl.fully_connected(h, self.z_dim, activation_fn = None)\n",
    "            logvar = tcl.fully_connected(h, self.z_dim, activation_fn = None)\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            return z, mu, logvar\n",
    "    \n",
    "    def decode(self, z, y, reuse = False):\n",
    "        with tf.variable_scope('decoder', reuse = reuse):\n",
    "            concat = tf.concat([z, y], 1)\n",
    "            h = tcl.fully_connected(concat, self.hidden_size, activation_fn = tf.nn.softplus)\n",
    "            mu     = tcl.fully_connected(h, self.x_dim, activation_fn = None)\n",
    "            logvar = tcl.fully_connected(h, self.x_dim, activation_fn = None)\n",
    "            return mu, logvar\n",
    "        \n",
    "    def likelihood(self, x, mu_x, logvar_x, y, z, mu_z, logvar_z):\n",
    "        # uniform prior\n",
    "        prior_y = (1. / self.num_labels) * tf.ones([tf.shape(x)[0], 10], tf.float32)\n",
    "        logpy = - tf.nn.softmax_cross_entropy_with_logits(logits = prior_y, labels = y)\n",
    "        \n",
    "        kld = tf.reduce_sum(logpdf.KLD(mu_z, logvar_z), 1)\n",
    "        logpx = tf.reduce_sum(logpdf.gaussian(x, mu_x, logvar_x), 1)\n",
    "        likelihood = logpx + logpy - kld  \n",
    "        return likelihood\n",
    "    \n",
    "    def prior_likelihood(self):\n",
    "        likelihood = 0\n",
    "        vars = self.trainable_vars()\n",
    "        for var in vars:\n",
    "            likelihood += tf.reduce_sum(logpdf.std_gaussian(var))\n",
    "        return likelihood\n",
    "    \n",
    "    def _build_graph(self, reuse = False):\n",
    "        self.x_l = tf.placeholder(tf.float32, shape = (None, self.x_dim))\n",
    "        self.y_l = tf.placeholder(tf.int32, shape = (None, ))\n",
    "        self.x_u = tf.placeholder(tf.float32, shape = (None, self.x_dim))\n",
    "        \n",
    "        self.y_l_onehot = tcl.one_hot_encoding(self.y_l, num_classes = self.num_labels)\n",
    "                \n",
    "        '''\n",
    "            classifier, labelled\n",
    "        '''\n",
    "        scores_l = self.classify(self.x_l, reuse = reuse)\n",
    "        # loss of classifier\n",
    "        self.loss_clf = tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                        logits = scores_l, labels = self.y_l_onehot )\n",
    "\n",
    "        '''\n",
    "            labelled data, encoder & decoder\n",
    "        '''\n",
    "        z_l, mu_z_l, logvar_z_l = self.encode(self.x_l, self.y_l_onehot, reuse = reuse)\n",
    "        mu_x_l, logvar_x_l = self.decode(z_l, self.y_l_onehot, reuse = reuse)\n",
    "        \n",
    "        # loss of labelled data, refered as L(x, y)\n",
    "        likelihood_l = self.likelihood(self.x_l, mu_x_l, logvar_x_l, \\\n",
    "                                       self.y_l_onehot, z_l, mu_z_l, logvar_z_l)\n",
    "        \n",
    "        '''\n",
    "            unlabelled data, encoder & decoder\n",
    "        '''\n",
    "        for i in range(self.num_labels):\n",
    "            y_us = i*tf.ones([tf.shape(self.x_u)[0]], tf.int32)\n",
    "            y_us = tcl.one_hot_encoding(y_us, num_classes = self.num_labels)\n",
    "            \n",
    "            z_u, mu_u, logvar_u = self.encode(self.x_u, y_us, reuse = True)\n",
    "            mu_recon_u, logvar_recon_u = self.decode(z_u, y_us, reuse = True)\n",
    "            \n",
    "            _likelihood_u = self.likelihood(self.x_u, mu_recon_u, logvar_recon_u,\\\n",
    "                                y_us, z_u, mu_u, logvar_u)\n",
    "            _likelihood_u = tf.expand_dims(_likelihood_u, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                likelihood_u = tf.identity( _likelihood_u )\n",
    "            else:\n",
    "                likelihood_u = tf.concat([likelihood_u, _likelihood_u], 1)\n",
    "            \n",
    "        # with x & clf, give the dist over y\n",
    "        scores_u = self.classify(self.x_u, reuse = True)\n",
    "        y_u_prob = tf.nn.softmax(scores_u, dim=-1)\n",
    "        \n",
    "        # add the H(q(y|x))\n",
    "        likelihood_u = tf.multiply(y_u_prob, likelihood_u + -tf.log(y_u_prob)) \n",
    "        likelihood_u = tf.reduce_sum(likelihood_u, 1)\n",
    "\n",
    "        alpha = 0.1 * self.batch_size\n",
    "        self.loss_clf = tf.reduce_sum(self.loss_clf, 0)\n",
    "        self.loss_l = - tf.reduce_sum(likelihood_l, 0)\n",
    "        self.loss_u = - tf.reduce_sum(likelihood_u, 0)\n",
    "        self.loss = (self.loss_l + alpha* self.loss_clf + self.loss_u)/self.batch_size\n",
    "\n",
    "        print('loss_u  : '+str(self.loss_u.shape))\n",
    "        print('loss_l  : '+str(self.loss_l.shape))\n",
    "        print('loss_clf: '+str(self.loss_clf.shape))\n",
    "        \n",
    "        prior_weight = 1./(self.dataset_size) \n",
    "        self.loss_prior = - self.prior_likelihood()\n",
    "        self.loss += prior_weight * self.loss_prior\n",
    "        \n",
    "        self.pred_y = tf.argmax(scores_u, 1)\n",
    "    \n",
    "    def _build_train_op(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable = False)\n",
    "        self.lr = tf.Variable(self.learning_rate, trainable=False, \n",
    "                    dtype=tf.float32)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        def ClipIfNotNone(grad):\n",
    "            if grad is None:\n",
    "                return grad\n",
    "            return tf.clip_by_value(grad, -1, 1)\n",
    "        capped_gvs = [(ClipIfNotNone(grad), var) for grad, var in grads_and_vars]\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs, self.global_step)\n",
    "        self.lr_decay_op = self.lr.assign(\n",
    "                self.lr * self.lr_decay_factor)\n",
    "    \n",
    "    def lr_decay(self, sess):\n",
    "        _ = sess.run([self.lr_decay_op])\n",
    "        \n",
    "    def _build_annealing(self):\n",
    "        self.kld_weight = tf.Variable(float(0.0), trainable=False, \n",
    "                                    dtype=tf.float32)\n",
    "        kld_anneal_factor = 0.95\n",
    "        self.anneal_decay_op = self.kld_weight.assign(\n",
    "                self.kld_weight * kld_anneal_factor)\n",
    "\n",
    "    def kld_anneal(self, sess):\n",
    "        _ = sess.run([self.anneal_decay_op])\n",
    "    \n",
    "    def predict(self, x, sess):\n",
    "        feed_dict = {\n",
    "            self.x_u: x,\n",
    "        }\n",
    "        pred = sess.run([self.pred_y], feed_dict = feed_dict)[0]\n",
    "        return pred\n",
    "    \n",
    "    def optimize(self, sess, x_l, y_l, x_u):\n",
    "        feed_dict = {\n",
    "            self.x_l: x_l,\n",
    "            self.y_l: y_l,\n",
    "\n",
    "            self.x_u: x_u,\n",
    "        }\n",
    "        eval_train = [self.train_op, self.loss]\n",
    "        eval_loss  = [self.loss_clf, self.loss_l, self.loss_u, self.loss_prior]\n",
    "        eval_vars = eval_train + eval_loss\n",
    "        _, loss, loss_clf, loss_l, loss_u, loss_p = sess.run(eval_vars, feed_dict = feed_dict)\n",
    "        return loss, loss_clf, loss_l, loss_u, loss_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_u  : ()\n",
      "loss_l  : ()\n",
      "loss_clf: ()\n",
      "SSL/classifier/fully_connected/weights:0: (784, 500)\n",
      "SSL/classifier/fully_connected/biases:0: (500,)\n",
      "SSL/classifier/fully_connected_1/weights:0: (500, 10)\n",
      "SSL/classifier/fully_connected_1/biases:0: (10,)\n",
      "SSL/encoder/fully_connected/weights:0: (794, 500)\n",
      "SSL/encoder/fully_connected/biases:0: (500,)\n",
      "SSL/encoder/fully_connected_1/weights:0: (500, 50)\n",
      "SSL/encoder/fully_connected_1/biases:0: (50,)\n",
      "SSL/encoder/fully_connected_2/weights:0: (500, 50)\n",
      "SSL/encoder/fully_connected_2/biases:0: (50,)\n",
      "SSL/decoder/fully_connected/weights:0: (60, 500)\n",
      "SSL/decoder/fully_connected/biases:0: (500,)\n",
      "SSL/decoder/fully_connected_1/weights:0: (500, 784)\n",
      "SSL/decoder/fully_connected_1/biases:0: (784,)\n",
      "SSL/decoder/fully_connected_2/weights:0: (500, 784)\n",
      "SSL/decoder/fully_connected_2/biases:0: (784,)\n",
      "\n",
      "time: 0 m 4 s\n",
      "epoch: 0\n",
      "  labelled loss: -406.02\n",
      "unlabelled loss: -187689.25\n",
      "classifier loss: 8.96\n",
      "     prior loss: 1527875.50\n",
      "     total loss: -344.74\n",
      " valid accuracy: 0.101\n",
      "  test accuracy: 0.097\n",
      "\n",
      "time: 0 m 44 s\n",
      "epoch: 10\n",
      "  labelled loss: -1220.96\n",
      "unlabelled loss: -514670.75\n",
      "classifier loss: 0.00\n",
      "     prior loss: 1531322.00\n",
      "     total loss: -1001.16\n",
      " valid accuracy: 0.492\n",
      "  test accuracy: 0.476\n",
      "\n",
      "time: 1 m 24 s\n",
      "epoch: 20\n",
      "  labelled loss: -1450.83\n",
      "unlabelled loss: -592951.81\n",
      "classifier loss: 0.00\n",
      "     prior loss: 1535346.62\n",
      "     total loss: -1158.10\n",
      " valid accuracy: 0.553\n",
      "  test accuracy: 0.528\n",
      "\n",
      "time: 2 m 5 s\n",
      "epoch: 30\n",
      "  labelled loss: -1387.33\n",
      "unlabelled loss: -675119.19\n",
      "classifier loss: 3.63\n",
      "     prior loss: 1539797.50\n",
      "     total loss: -1321.85\n",
      " valid accuracy: 0.575\n",
      "  test accuracy: 0.545\n",
      "\n",
      "time: 2 m 46 s\n",
      "epoch: 40\n",
      "  labelled loss: -1320.18\n",
      "unlabelled loss: -714926.25\n",
      "classifier loss: 0.06\n",
      "     prior loss: 1545309.50\n",
      "     total loss: -1401.58\n",
      " valid accuracy: 0.575\n",
      "  test accuracy: 0.547\n",
      "\n",
      "time: 3 m 27 s\n",
      "epoch: 50\n",
      "  labelled loss: -1542.88\n",
      "unlabelled loss: -762359.56\n",
      "classifier loss: 0.00\n",
      "     prior loss: 1550608.50\n",
      "     total loss: -1496.79\n",
      " valid accuracy: 0.574\n",
      "  test accuracy: 0.547\n",
      "\n",
      "time: 4 m 8 s\n",
      "epoch: 60\n",
      "  labelled loss: -1222.98\n",
      "unlabelled loss: -780077.25\n",
      "classifier loss: 0.00\n",
      "     prior loss: 1556339.00\n",
      "     total loss: -1531.47\n",
      " valid accuracy: 0.571\n",
      "  test accuracy: 0.543\n",
      "\n",
      "time: 4 m 49 s\n",
      "epoch: 70\n",
      "  labelled loss: -1585.35\n",
      "unlabelled loss: -789719.25\n",
      "classifier loss: 4.93\n",
      "     prior loss: 1562901.25\n",
      "     total loss: -1550.86\n",
      " valid accuracy: 0.560\n",
      "  test accuracy: 0.529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils import (load_mnist_split, create_semisupervised, \\\n",
    "                   ssl_batch_gen, time_since, get_dims, sample)\n",
    "from vae import VAE\n",
    "\n",
    "'''\n",
    "    here are the config params for the experiment.\n",
    "'''\n",
    "data_size = 50000\n",
    "batch_size = 500\n",
    "n_labelled = 100\n",
    "n_epoch = 1000\n",
    "\n",
    "# load data from mnist\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = load_mnist_split()\n",
    "# split training set\n",
    "X_labelled, Y_labelled, X_unlabelled, Y_unlabelled = create_semisupervised(\\\n",
    "                                                        train_x, train_y, n_labelled)\n",
    "\n",
    "np.random.seed(31415)\n",
    "tf.set_random_seed(31415)\n",
    "\n",
    "data_x_l = X_labelled\n",
    "data_x_u = X_unlabelled\n",
    "data_y_l = Y_labelled\n",
    "\n",
    "# Set config for tensorflow session.\n",
    "tf_config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 1}, # single gpu\n",
    ")\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "\n",
    "x_dim = data_x_u.shape[1]\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session(config = tf_config) as sess:\n",
    "        model = SSL(x_dim = x_dim)\n",
    "        var_list = model.global_vars()\n",
    "        init_op = tf.variables_initializer(var_list)\n",
    "        sess.run(init_op)\n",
    "\n",
    "        start = time.time()\n",
    "        for epoch in range(10000):\n",
    "            l_batch_gen, u_batch_gen = ssl_batch_gen(data_x_l, data_y_l, data_x_u, 500, 1)\n",
    "\n",
    "            for l_batch, u_batch in zip(l_batch_gen, u_batch_gen):\n",
    "                x_l, y_l = zip(*l_batch)\n",
    "                x_u = zip(*u_batch)[0]\n",
    "                loss, loss_clf, loss_l, loss_u, loss_p = model.optimize(sess, x_l, y_l, x_u)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                pred_valid = model.predict(valid_x, sess)\n",
    "                pred_test  = model.predict(test_x,  sess)\n",
    "                accuracy_valid = accuracy_score(valid_y, pred_valid)\n",
    "                accuracy_test  = accuracy_score(test_y,  pred_test)\n",
    "\n",
    "                print('time: %s' % time_since(start))\n",
    "                print('epoch: %d' % epoch)\n",
    "                print('  labelled loss: %.2f' % loss_l)\n",
    "                print('unlabelled loss: %.2f' % loss_u)\n",
    "                print('classifier loss: %.2f' % loss_clf)\n",
    "                print('     prior loss: %.2f' % loss_p)\n",
    "                print('     total loss: %.2f' % loss)\n",
    "                print(' valid accuracy: %.3f' % accuracy_valid)\n",
    "                print('  test accuracy: %.3f' % accuracy_test)\n",
    "                print()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                model.lr_decay(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
